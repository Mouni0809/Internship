{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec001277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mounisha s\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mounisha s\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038b0751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mounisha s\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mounisha s\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mounisha s\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mounisha s\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mounisha s\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ecd20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_headers_to_dataframe(url):\n",
    "    # Send an HTTP GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to retrieve the web page\")\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all header tags (h1, h2, h3, h4, h5, h6)\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    # Extract the text content from the header tags\n",
    "    headers_text = [header.get_text() for header in header_tags]\n",
    "\n",
    "    # Create a DataFrame from the extracted header data\n",
    "    df = pd.DataFrame({'Headers': headers_text})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "    header_df = scrape_headers_to_dataframe(url)\n",
    "    print(header_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95e83db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the web page. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the web page you want to scrape\n",
    "url = \"https://www.example.com/sample-page\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the elements containing the data you want to scrape\n",
    "    # You should inspect the actual web page to determine the correct HTML structure\n",
    "    # and identify the elements that contain the presidential information\n",
    "    # For demonstration purposes, we will assume a hypothetical HTML structure\n",
    "    president_elements = soup.find_all(\"div\", class_=\"president\")\n",
    "\n",
    "    # Create empty lists to store the extracted data\n",
    "    names = []\n",
    "    terms = []\n",
    "\n",
    "    # Extract the data from the elements\n",
    "    for president in president_elements:\n",
    "        name = president.find(\"span\", class_=\"name\").text\n",
    "        term = president.find(\"span\", class_=\"term\").text\n",
    "        names.append(name)\n",
    "        terms.append(term)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    data = {\"Name\": names, \"Term of Office\": terms}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "    # You can save the DataFrame to a CSV file if needed\n",
    "    # df.to_csv(\"presidents.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d22fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      27  3,112    115\n",
      "1     Pakistan\\nPAK      27  3,102    115\n",
      "2        India\\nIND      40  4,558    114\n",
      "3      England\\nENG      28  2,942    105\n",
      "4  South Africa\\nSA      23  2,386    104\n",
      "5   New Zealand\\nNZ      31  3,110    100\n",
      "6   Bangladesh\\nBAN      33  3,107     94\n",
      "7     Sri Lanka\\nSL      37  3,448     93\n",
      "8  Afghanistan\\nAFG      21  1,687     80\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_mens_rankings_to_dataframe(url):\n",
    "    # Send an HTTP GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to retrieve the web page\")\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Initialize lists to store team data\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through the table rows and extract data for the top 10 teams\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 5:\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Team': teams,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    odi_mens_rankings_df = scrape_odi_mens_rankings_to_dataframe(url)\n",
    "    print(odi_mens_rankings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ba4251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0           Shubman Gill  IND    759\n",
      "1  Rassie van der Dussen   SA    745\n",
      "2           David Warner  AUS    739\n",
      "3            Imam-ul-Haq  PAK    735\n",
      "4           Harry Tector  IRE    726\n",
      "5        Quinton de Kock   SA    721\n",
      "6            Virat Kohli  IND    715\n",
      "7           Rohit Sharma  IND    707\n",
      "8           Fakhar Zaman  PAK    705\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_mens_batsmen_rankings_to_dataframe(url):\n",
    "    # Send an HTTP GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to retrieve the web page\")\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Initialize lists to store batsman data\n",
    "    batsmen = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through the table rows and extract data for the top 10 batsmen\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 3:\n",
    "            batsman = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            batsmen.append(batsman)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Batsman': batsmen,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"\n",
    "    odi_mens_batsmen_rankings_df = scrape_odi_mens_batsmen_rankings_to_dataframe(url)\n",
    "    print(odi_mens_batsmen_rankings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9539e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Bowler Team Rating\n",
      "0    Josh Hazlewood  AUS    692\n",
      "1    Mitchell Starc  AUS    666\n",
      "2       Trent Boult   NZ    666\n",
      "3        Adam Zampa  AUS    663\n",
      "4        Matt Henry   NZ    658\n",
      "5  Mujeeb Ur Rahman  AFG    657\n",
      "6     Kuldeep Yadav  IND    656\n",
      "7       Rashid Khan  AFG    655\n",
      "8    Mohammed Siraj  IND    643\n",
      "9    Shaheen Afridi  PAK    635\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_mens_bowlers_rankings_to_dataframe(url):\n",
    "    # Send an HTTP GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to retrieve the web page\")\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Initialize lists to store bowler data\n",
    "    bowlers = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through the table rows and extract data for the top 10 bowlers\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 3:\n",
    "            bowler = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            bowlers.append(bowler)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Bowler': bowlers,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "    odi_mens_bowlers_rankings_df = scrape_odi_mens_bowlers_rankings_to_dataframe(url)\n",
    "    print(odi_mens_bowlers_rankings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffcc52f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      26  4,290    165\n",
      "1      England\\nENG      31  3,875    125\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      30  3,039    101\n",
      "4   New Zealand\\nNZ      28  2,688     96\n",
      "5   West Indies\\nWI      29  2,743     95\n",
      "6   Bangladesh\\nBAN      17  1,284     76\n",
      "7     Sri Lanka\\nSL      12    820     68\n",
      "8     Thailand\\nTHA      13    883     68\n",
      "9     Pakistan\\nPAK      27  1,678     62\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_womens_teams_rankings_to_dataframe(url):\n",
    "    # Send an HTTP GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to retrieve the web page\")\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Initialize lists to store team data\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through the table rows and extract data for the top 10 teams\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 5:\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Team': teams,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "    odi_womens_teams_rankings_df = scrape_odi_womens_teams_rankings_to_dataframe(url)\n",
    "    print(odi_womens_teams_rankings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505397bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    801\n",
      "1           Beth Mooney  AUS    751\n",
      "2   Chamari Athapaththu   SL    743\n",
      "3       Laura Wolvaardt   SA    708\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    702\n",
      "6      Harmanpreet Kaur  IND    694\n",
      "7          Ellyse Perry  AUS    686\n",
      "8           Meg Lanning  AUS    682\n",
      "9       Stafanie Taylor   WI    618\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_batting_rankings_to_dataframe(url):\n",
    "    # Send an HTTP GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to retrieve the web page\")\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Initialize lists to store batsman data\n",
    "    batsmen = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through the table rows and extract data for the top 10 batsmen\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 3:\n",
    "            batsman = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            batsmen.append(batsman)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Batsman': batsmen,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "    womens_odi_batting_rankings_df = scrape_womens_odi_batting_rankings_to_dataframe(url)\n",
    "    print(womens_odi_batting_rankings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01f0314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            All-Rounder Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    398\n",
      "1      Ashleigh Gardner  AUS    389\n",
      "2       Hayley Matthews   WI    382\n",
      "3        Marizanne Kapp   SA    362\n",
      "4          Ellyse Perry  AUS    329\n",
      "5           Amelia Kerr   NZ    328\n",
      "6         Deepti Sharma  IND    312\n",
      "7         Jess Jonassen  AUS    241\n",
      "8         Sophie Devine   NZ    233\n",
      "9              Nida Dar  PAK    217\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_allrounders_rankings_to_dataframe(url):\n",
    "    # Send an HTTP GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to retrieve the web page\")\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Initialize lists to store all-rounder data\n",
    "    allrounders = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through the table rows and extract data for the top 10 all-rounders\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 3:\n",
    "            allrounder = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            allrounders.append(allrounder)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'All-Rounder': allrounders,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "    womens_odi_allrounders_rankings_df = scrape_womens_odi_allrounders_rankings_to_dataframe(url)\n",
    "    print(womens_odi_allrounders_rankings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00db96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all the headline elements on the page\n",
    "    headlines = soup.find_all('h3', class_='Card-title')\n",
    "\n",
    "    # Extract headline text and store it in a list\n",
    "    headline_texts = [headline.text.strip() for headline in headlines]\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({'Headline': headline_texts})\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "755575dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Timestamp]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the CNBC World News page\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the news articles on the page\n",
    "    news_articles = soup.find_all(\"div\", class_=\"Card-title\")\n",
    "\n",
    "    # Create empty lists to store the news headlines and timestamps\n",
    "    headlines = []\n",
    "    timestamps = []\n",
    "\n",
    "    # Iterate through the news articles and extract headlines and timestamps\n",
    "    for article in news_articles:\n",
    "        headline = article.text.strip()\n",
    "        timestamp = article.find_previous(\"time\").text.strip()\n",
    "        headlines.append(headline)\n",
    "        timestamps.append(timestamp)\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame({'Headline': headlines, 'Timestamp': timestamps})\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f6c251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Timestamp, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the CNBC World News page\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the news articles on the page\n",
    "    news_articles = soup.find_all(\"div\", class_=\"Card-title\")\n",
    "\n",
    "    # Create empty lists to store the news headlines, timestamps, and news links\n",
    "    headlines = []\n",
    "    timestamps = []\n",
    "    news_links = []\n",
    "\n",
    "    # Iterate through the news articles and extract headlines, timestamps, and links\n",
    "    for article in news_articles:\n",
    "        headline = article.text.strip()\n",
    "        timestamp = article.find_previous(\"time\").text.strip()\n",
    "        link = article.find(\"a\")[\"href\"]\n",
    "        full_link = f\"https://www.cnbc.com{link}\"\n",
    "        headlines.append(headline)\n",
    "        timestamps.append(timestamp)\n",
    "        news_links.append(full_link)\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame({'Headline': headlines, 'Timestamp': timestamps, 'News Link': news_links})\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eb8178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the URL of the most downloaded articles page\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the articles list\n",
    "    articles = soup.find_all('div', class_='article-details')\n",
    "    \n",
    "    # Create lists to store the scraped data\n",
    "    paper_titles = []\n",
    "    authors_list = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "\n",
    "    # Define the date range for the last 90 days\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=90)\n",
    "    \n",
    "    # Loop through the articles and scrape the required details\n",
    "    for article in articles:\n",
    "        # Get the publication date of the article\n",
    "        date_str = article.find('span', class_='published-online').text.strip()\n",
    "        published_date = datetime.strptime(date_str, '%d %B %Y')\n",
    "        \n",
    "        # Check if the article was published in the last 90 days\n",
    "        if start_date <= published_date <= end_date:\n",
    "            # Get the paper title\n",
    "            paper_title = article.find('h2', class_='article-title').text.strip()\n",
    "            paper_titles.append(paper_title)\n",
    "\n",
    "            # Get the authors' names\n",
    "            authors = article.find('span', class_='authors').text.strip()\n",
    "            authors_list.append(authors)\n",
    "\n",
    "            # Store the publication date\n",
    "            published_dates.append(published_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "            # Get the paper URL\n",
    "            paper_url = article.find('a', class_='anchor-link')['href']\n",
    "            paper_urls.append(paper_url)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    data = {\n",
    "        'Paper Title': paper_titles,\n",
    "        'Authors': authors_list,\n",
    "        'Published Date': published_dates,\n",
    "        'Paper URL': paper_urls\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34fddf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the dineout.co.in page you want to scrape\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the restaurant listings\n",
    "    restaurant_listings = soup.find_all('div', class_='restnt-info')\n",
    "    \n",
    "    # Create lists to store the scraped data\n",
    "    restaurant_names = []\n",
    "    cuisines = []\n",
    "    locations = []\n",
    "    ratings = []\n",
    "    image_urls = []\n",
    "\n",
    "    # Loop through the restaurant listings and scrape the required details\n",
    "    for listing in restaurant_listings:\n",
    "        # Get the restaurant name\n",
    "        restaurant_name = listing.find('span', class_='restnt-card-title').text.strip()\n",
    "        restaurant_names.append(restaurant_name)\n",
    "\n",
    "        # Get the cuisine type\n",
    "        cuisine = listing.find('span', class_='double-line-ellipsis').text.strip()\n",
    "        cuisines.append(cuisine)\n",
    "\n",
    "        # Get the restaurant location\n",
    "        location = listing.find('span', class_='restnt-loc-text').text.strip()\n",
    "        locations.append(location)\n",
    "\n",
    "        # Get the restaurant rating\n",
    "        rating = listing.find('span', class_='restnt-rating-text').text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "        # Get the image URL\n",
    "        image_url = listing.find('img', class_='no-webp-img')['data-src']\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    data = {\n",
    "        'Restaurant Name': restaurant_names,\n",
    "        'Cuisine': cuisines,\n",
    "        'Location': locations,\n",
    "        'Ratings': ratings,\n",
    "        'Image URL': image_urls\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c85ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
